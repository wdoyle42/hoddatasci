---
title: "Classification"
author: "Will Doyle"
date: "10/17/2016"
output: github_document
---

Classification is the process of predicting group membership. Understanding which individuals are likely to be members of which groups is a key task for data scientists. For instance, most recommendation engines that are at the hear of consumer web sites are based on classification algorithms, predicting which consumers are likely to purchase which products. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Pizza

Today we'll be working with the pizza dataset, which comes from the subreddit random acts of pizza. Each line represents a post to this subreddit. We have various characteristics of these posts, along with the request text from the post itself. We'll use these characteristics of the posts to predict whether or not the poster received pizza. 

```{r libraries}
library(tidyverse)
library(ggplot2)
library(caret)
```

```{r data}
za<-read_csv("pizza.csv")
```

Below, we do some basic data wrangling, changing variable names and recoding a few variables to be in a more usable format. 

```{r wrangling}

#Recoding
za$got_pizza<-rep(0,dim(za)[1])
za$got_pizza[za$requester_received_pizza=="True"]<-1

## Renaming looooong variable names
za$karma<-za$requester_upvotes_minus_downvotes_at_request

za$age<-za$requester_account_age_in_days_at_request

za$raop.age<-za$requester_days_since_first_post_on_raop_at_request

za$pop.request<-za$number_of_upvotes_of_request_at_retrieval

za$activity<-za$requester_number_of_subreddits_at_request

za$total.posts<-za$requester_number_of_posts_at_request

za$raop.posts<-za$requester_number_of_posts_on_raop_at_request

# Binary variable for previous post
za$prev.raop.post<-0

za$prev.raop.post[za$raop.posts>0]<-1

# Binary variable: word "student" in text
za$student<-0

za$student[grep("student",za$request_text)]<-1

# Binary variable: word "poor in text"
za$poor<-0

za$poor[match(c("poor","money"),za$request_text)]<-1

# Binary variable: word "grateful"" in text
za$grateful<-0

za$grateful[grep("grateful",za$request_text)]<-1

```

We'll start by generating some cross tabs and some quick plots, showing the probability of receiving pizza according to several characteristics of the post.  We start with a basic crosstab of the dependent variable. We use `prop.table` to change this from raw counts to proportions. 

```{r descriptives}
#Cross Tabs

table(za$got.pizza)

prop.table(table(za$got.pizza))
```

So, about 75% of the sample didn't get pizza, about 25% did. 

Next, we cross-tabulate receiving pizza with certain terms. First, if the request mentioned the word "student."

```{r}
s.table<-table(za$student,za$got.pizza);s.table

prop.table(s.table,margin=1)

p.table<-table(za$poor,za$got.pizza);p.table

prop.table(p.table,margin=1)
```

Next, if the request mentioned the word "grateful."

```{r}
g.table<-table(za$grateful,za$got.pizza);g.table

prop.table(g.table,margin=1)
```

Crosstabs using binary data are equivalent to generating conditional means, as shown below. 

```{r condtional_means}
#Predictions using conditional means

za%>%group_by(grateful)%>%summarize(mean(got.pizza))

```

But, we can also use conditional means to get proportions for very particular sets of characteristics. In this case, what about individuals who included some combination of the terms "grateful","student" and "poor" in their posts? 

```{r}

za%>%group_by(grateful,student)%>%summarize(mean(got.pizza))

za%>%group_by(grateful,student)%>%summarize(sum(got.pizza))

za_sum<-za%>%group_by(grateful,student,poor)%>%summarize(mean_pizza=mean(got.pizza))

za_sum

```

## Probability of Receiving Pizza, Using Various Terms in Post
```{r}
gg<-ggplot(za_sum,aes(x=grateful,y=mean_pizza))
gg<-gg+geom_bar(stat="identity")
gg<-gg+facet_wrap(~student+poor)
gg
```

## Classiciation Using Linear Probability Model

We can use standard OLS regression for classification. It's not ideal, but most of the time it's actually not too bad, either. Below we model the binary outcome of receiving pizza as a function of karma, total posts, posts on the pizza subreddit, wehterh or not the poster mentioned the words "student" or "grateful."

```{r linear_model}
# Linear model

lm.mod<-lm(got.pizza~age+
             karma+
             log(total.posts+1)+
             raop.posts+
             student+
             grateful,data=za,y=TRUE,na.exclude=TRUE);summary(lm.mod)
```

We're going to do something a bit different with the predictions from this model. After creating predictions, we're going to classify everyone with a predicted probablity above .5 as being predicted to get a pizza, while everyone with a predicted probability below .5 is predicted to not get one. We'll compare our classifications with the actual data. 

```{r}
#Predictions

lm.predict<-predict(lm.mod)

lm.predict.bin<-rep(0,length(lm.predict))

lm.predict.bin[lm.predict>=.5]<-1

lm.table<-table(lm.predict.bin,lm.mod$y)

pcp<-(lm.table[1,1]+lm.table[2,2])/sum(lm.table)

pred_table<-prop.table(lm.table,margin=1)

rownames(pred_table)<-c("Predicted: Yes","Predicted: No")
colnames(pred_table)<-c("Actual: Yes","Actual: No")

confusionMatrix(data=lm.predict.bin,reference = lm.mod$y,positive="1")
```

The confusion matrix generated here is explained [here](https://topepo.github.io/caret/measuring-performance.html#class). 


```{r roc_auc}
lm.roc<-roc(lm.predict,as.factor(lm.mod$y))

auc(lm.roc)
```


We're usually interested in three things: the overall accuracy of a classification is the proportion of cases accurately classified. The sensitivity is the proportion of "ones" that are accurately classified as ones-- it's the probability that a case classified as positive will indeed be positive. Specificity is the probability that a case classified as 0 will indeed by classified as 0. 

There are several well-known problems with linear regression as a classification algortihm. Two should give us pause: it can generate probabilites outside of 0,1 and it implies a linear change in probabilities as a function of the predictors which may not be justified given the underlying relationship between the predictors and the probability that the outcome is 1. Logistic regresssion should give a better predicted probability, one that's more sensitive to the actual relationship between the predictors and the outcome. 

```{r}
#Logisitic model

logit.mod<-glm(got.pizza~
               data=za,
               na.action=na.exclude,
               family=binomial(link="logit"),
               y=TRUE)

summary(logit.mod)
```

With these results in hand we can generate predicted probabilities and see if this model did any better. 

```{r}
logit_predict<-predict(logit.mod,type="response")

logit.predict.bin<-rep(0,length(logit_predict))

logit.predict.bin[logit_predict>=.5]<-1

logit.table<-table(logit.predict.bin,za$got.pizza)

pcp_logit<-(logit.table[1,1]+logit.table[2,2])/sum(logit.table)

pred_table<-prop.table(lm.table,margin=1)

rownames(pred_table)<-c("Predicted: Yes","Predicted: No")

colnames(pred_table)<-c("Actual: Yes","Actual: No")

confusionMatrix(data=logit.predict.bin,reference=za$got.pizza,positive="1")

logit.roc<-roc(logit_predict,as.factor(logit.mod$y))

auc(logit.roc)

plot(logit.roc)

```

```{r}

logit.mod.2<-glm(got.pizza~
               data=za,
               na.exclude=TRUE,
               y=TRUE)

summary(logit.mod.2)

logit_pred_2<-predict(logit.mod.2,type="response")

logit_pred_2_bin<-rep(0,length(logit_pred_2))
logit_pred_2_bin[logit_pred_2>.5]<-1

confusionMatrix(data=logit_pred_2_bin,reference=logit.mod.2$y,positive="1")

```

## Predicted Probability

```{r}

za_preds<-za%>%select(karma,got.pizza,student)
za_preds$predict<-logit_predict
ze_preds<-za_preds%>%arrange(age)

gg<-ggplot(za_preds,aes(x=karma,y=got.pizza))
gg<-gg+geom_jitter(alpha=.25,size=.5,position=position_jitter(height=.4,width=.4))
gg<-gg+scale_x_continuous(trans="log",breaks=c(0,10,100,1000,3000,10000))
gg<-gg+geom_smooth(data=za_preds,aes(x=karma,y=predict,color=as.factor(student)))

gg



za$words<-str_count(za$request_text,"\\S+")


```