---
title: "Web Scraping"
author: "Will Doyle"
output:
  html_document: default
  pdf_document: default
---

## Introduction

Many large web sites host a huge amount of information. This information is encoded and delivered on demand to the user within a web page, which is really just a markup language that a browser can understand. We can take this data and analyze it using R, via a variety of different means. Today we'll cover scraping web tables and interacting via Automated Programming Interfaces.

## Ethics and Responsiblity

Many of the tools we'll cover can be quite powerful ways to interact with data stored online and gather it for analysis. Because they're powerful, you need to be careful with them. In particular, try to request information in a way that will not burden the website owners. What constitutes a burden depends on the website. Google, Twitter, Facebook, all of the big websites have protections in place to guard against too many requests and have a huge amount of capacity for taking the requests they grant. Smaller websites may not have either. Always strive to be minimally intrusive: you're usually getting this data for free. 

## Ways of getting data from the web

We will cover several different ways you can get data from the web

1. Directly downloading web pages via the `url()` command. 
1. Scraping simple web tables via `read_html()` and `html_table()` command
1. Scraping complex web tables via `read_html()` and `html_nodes()`
1. Interacting with Application Programming Interfaces (APIs) via R libraries that have been designed as "wrappers" for these interfaces, like the awesome `acs` library and the `tigris` library for geographic shapes. 
1. Interacting with APIs directly, using the [Zillow API](https://www.zillow.com/howto/api/APIOverview.htm).

Just for fun, we'll output the results of this data to interactive graphics. 

## Libraries

We will use multiple new libraries today. Among the ones you'll need: 

* `rvest` for scraping websites

* `lubridate` for working with dates

* `plotly` for interactive plots

* `acs` for accessing American Community Survey data via the census API

* `tigris` for accessing TIGER shapefiles via the census API

* `noncensus` for various geographic information, particularly around zip codes. 

* `tidycensus`

```{r}
library(rvest)
library(ggthemes)
library(lubridate)
library(acs)
library(noncensus)
library(leaflet)
library(tigris)
options(tigris_use_cache = TRUE) # save tigris shapefiles
library(plotly)
library(ggplot2)
library(forcats)
library(tidycensus)
library(tidyverse)
```


## API keys

You will also need two API keys. 

* The Census API, available here: https://api.census.gov/data/key_signup.html 

* And Zillow's API key, available here: https://www.zillow.com/howto/api/APIOverview.htm

# Basics of interacting with information stored online

R can understand a web connection via the `url` command. Once that connection is established, we can download whatever we'd like. 

```{r}

#Web connections: url
# example
r_home = url("http://www.r-project.org/")
r_home

# Pulling text from a website using readlines
# url of Moby Dick (project Gutenberg)
moby_url = url("http://www.gutenberg.org/files/2701/2701-h/2701-h.htm")
# reading the content (first 500 lines)
moby_dick = readLines(moby_url, n = 5000)
moby_dick[1175:1210]
```


# Scraping web tables

When we talk about "scraping" a web table, we're talking about pulling a table that exists on a website and turning it into a usable data frame for analysis. Below, I take the table from  `http://en.wikipedia.org/wiki/Marathon_world_record_progression` for men's marathon times and plot the change in speed in m/s as a function of the date that the world record was set. 

```{r}
marathon_wiki = "https://en.wikipedia.org/wiki/Marathon_world_record_progression"

# Get the page, pull the tables via html_table
marathon <- read_html(marathon_wiki)%>%html_table(fill=TRUE)

#Men's is the first table
marathon<-as_tibble(marathon[[2]])

#Convert time to seconds
marathon<-marathon%>%
  mutate(Time2=hms(as.character(Time)))%>%
  mutate(Time2=period_to_seconds(Time2))

#Marathons are 42,200 meters long
marathon$speed<-(4.22e4)/marathon$Time2

#Get dates in a usable format usin lubridate::mdy
marathon$date<-mdy(marathon$Date)
```



## Progression of World Record Marathon Speed in Meters/Second
```{r}
marathon<-marathon%>%mutate(Nationality=fct_reorder(.f=as.factor(Nationality),
                                                    .x=-speed,fun = max))
g1<-ggplot(data=marathon,
           aes(y=speed,x=date,
               #Reorder nationality by fastest times
               color=Nationality,
               text=paste(Name,Time))
           )  
g1<-g1+geom_point()+
           xlab("Date")+
           ylab("Meters/Second")+
           theme_igray()

g1
##ggplotly(g1)

```

_Quick Exercise_ Repeat the above analysis for women's world record progression.

# Interacting with more complex websites

To get data from a more complex site will take several steps. First, you need to figure out the strucutre for the url naming. Then you need to use something like [selector gadget](http://selectorgadget.com/) to access the data. We'll go over this in class. 

Below, I access some team data from the nba.com website. To get the table in shape, I needed to figure out the "node" associate with each column. This took some time. It turns out that each column in the table `.left` or `.right` is named `nth-child(#)` where `#` is a number from 1-30. 

## The first step: get the page

I first plug the web page I'm interested in to a variable called url. This makes it simpler to deal with in code. 

```{r}
## Web Page: could be lots of different things. 
url<-"http://www.basketball-reference.com/leagues/NBA_2018_totals.html"

```

The next step is to download that web page onto my computer. I call this `page` and I use the `read_html()` function to get it. 

```{r}
page<-read_html(url)
```


The web page has all of the data we need. The trick is to figure out how it's organized. To do this I need to use a tool called `selectorgadget`. What this will do is provide me with the name of the element I'm highlighting. I can also type in element names and see what's highlighted. After some trial and error, I figured out that there are two main elements to the table on this page `.left` and `.right`. `.left` contains player names and positions, while `.right` has all of the relevant data for that player. Using the `html_nodes()` command, I extract player names from `.left`. 

![selector_gadget](./nba_totals.png)

```{r}
## Get data frame started: fill in player names
my_nodes<-paste0(".left:nth-child(",2,")")

## This code gets the data !
player_name<-page%>%
  html_nodes(my_nodes)%>%
  html_text()
  
player_name[1:10]
```

Using the same logic, I grab their points from the table
```{r}
## Get data frame started: fill in player names
my_nodes<-paste0(".right:nth-child(",30,")")

## This code gets the data !
points<-page%>%
  html_nodes(my_nodes)%>%
  html_text()
  
points[1:10]

my_nodes<-paste0(".right:nth-child(",9,")")

## This code gets the data !
fg<-page%>%
  html_nodes(my_nodes)%>%
  html_text()
  
fg[1:10]

```


Now I can combine the two into a data frame. Note that I have to drop the last row of player names-- the web page had an ad there. 
```{r}
#player_name<-player_name[-length(player_name)]

simple_data<-tibble(player_name,points,fg)
```

Next we can change the points variable to be numeric, and arrange in descending order. 

```{r}
simple_data<-simple_data%>%mutate_at("points",as.numeric)

simple_data<-simple_data%>%arrange(-points)

simple_data
```

## Putting it all together

The code below takes the exact same steps, but expands in two directions. First, it repeats over the years 1993-2017 to get a 25 year dataset. Next, it includes all of the columnns in the table from the NBA reference website. 

```{r}
## Pulling from a "stats" website

#Check: do you really want to get data?

get_nba_data<-FALSE

#initialize full dataset
nba_df_all<-NULL

#What years?
year_list<-1993:2019

if(get_nba_data==TRUE){

  ## Loop through years
for (year in year_list){

## Web Page: could be lots of different things. 
url<-paste0("http://www.basketball-reference.com/leagues/NBA_",year,"_totals.html")

nba_df<-url %>% 
      read_html() %>%                   # parse html
      html_nodes('#all_totals_stats')%>% # Select total stats table
      html_node('table')%>% # Grab just the table
      html_table()%>% # pull the table element in a format readable by R
      data.frame() #output to data frame

## Names from the web page
nba_names<-c("rank",
             "player",
             "position",
             "age",
             "team",
             "games",
             "games_started",
             "minutes_played",
             "fg",
             "fg_attempts",
             "fg_percent",
             "three_pointers",
             "three_point_attempts",
             "three_point_percent",
             "two_pointers",
             "two_point_attempts",
             "two_point_percent",
             "effective_fg_percent",
             "free_throws",
             "free_throw_attempts",
             "free_throw_percent",
             "off_rebound",
             "def_rebound",
             "total_rebound",
             "assists",
             "steals",
             "blocks",
             "turnovers",
             "fouls",
             "pts"
)

names(nba_df)<-nba_names

## Add year, since we'll be including multiple years
nba_df$year<-year

#Combine into full dataset
nba_df_all<-bind_rows(nba_df_all,nba_df)

# TOS ask for a 3s crawl delay: No Problem! https://www.sports-reference.com/robots.txt
## Wait between 3 and 5 seconds before going back for next season (being polite)
Sys.sleep(runif(1,3,4))
print(paste("Completed year", year))

}#end year loop

nba_df_all<-nba_df_all%>%
  mutate_at(.funs=as.numeric,.vars=names(nba_df_all)[6:31]) %>% #convert all to numeric
  filter(!(is.na(games))) #drop "marker" rows
  
save(nba_df_all,file = "nba.Rdata")

}else{
  load("nba.Rdata")
}    #End conditional

```

## Points per minute and three pointers

Points per minute is one of the key measures of an NBA player's output. Players with points per minute above .6 are considered in the league's elite. The way that players achieve this has changed over time. In particular, players have relied much more on three-point shooting to achieve this benchmark. The graphic below shows points per minute as a function of the number of three pointers scored in a season. In 2001, top producers like Shaquille O'Neal shot no three pointers. By 2015, almost all of the top producers like LeBron James, Steph Curry and Kevin Durant relied on three-point shots as a large part of their production. 
```{r}
nba_df_all<-nba_df_all%>%mutate(pts_minute=pts/minutes_played)

gg<-ggplot(filter(nba_df_all,pts>1000,year>1992),aes(y=pts_minute,x=three_pointers,
                          text=player,
                          color=as.factor(year)))
gg<-gg+geom_hline(yintercept=.6,linetype="dashed",alpha=.3)
gg<-gg+geom_point(size=.5,alpha=.75)
gg<-gg+facet_wrap(~year)
gg<-gg+theme(legend.position="none")+xlab("Three Pointers")+ylab("Pts/Minute")
ggplotly(gg)

nba_df_all%>%
  filter(year==1998)%>%
  lm(pts_minute~I(three_pointers/100),data=.)%>%
  summary()

#Predicting points per minute by three pointers, 1998
nba_mod_1998<-lm(pts_minute~I(three_pointers/100),
                 data=dplyr::filter(nba_df_all,year==1998));summary(nba_mod_1998)

#Predicting same in 2017
nba_mod_2018<-lm(pts_minute~I(three_pointers/100),
                 data=dplyr::filter(nba_df_all,year==2018));summary(nba_mod_2018)
```


```{r}
gg<-ggplot(filter(nba_df_all,pts>500,year>1993),aes(y=pts,x=free_throws,
                          text=player,
                          color=as.factor(year)))
#gg<-gg+geom_hline(yintercept=.6,linetype="dashed",alpha=.3)
gg<-gg+geom_point(size=.5,alpha=.75)
gg<-gg+facet_wrap(~year)
gg<-gg+theme(legend.position="none")+xlab("Free Throws")+ylab("Total Points")
ggplotly(gg)

```


## Working with NFL Reference

```{r}

nfl_get_data<-FALSE

nfl_df_all<-NULL

if (nfl_get_data==TRUE){

  for (year in 2008:2018){
url<-paste0("https://www.pro-football-reference.com/years/",year,"/")

## Parse web page
nfl_df<-url %>% read_html() %>%                   # parse html
      html_nodes('#all_team_stats')%>%
      html_nodes(xpath='comment()')%>%
      html_text()%>%
      read_html()%>%
      html_node('table')%>%
      html_table()
  
nfl_names<-c("rank",
             "team",
             "games",
             "points_for_offense",
             "yards",
             "plays",
             "yards/play",
             "turnovers_lost",
             "fumbles_lost",
             "first_downs",
             "pass_completions",
             "pass_attempts",
             "pass_yards",
             "pass_tds",
             "pass_interceptions_against",
             "pass_net_yards_attempt",
             "pass_first_downs",
             "rush_attempts",
             "rush_yards",
             "rush_tds",
             "rush_yards_attempt",
             "rush_first_downs",
             "penalties_commit",
             "penalty_yards_commit",
             "first_downs_by_penaly",
             "drive_score_pct",
             "drive_to_pct",
             "expected_points")

names(nfl_df)<-nfl_names

nfl_df<-nfl_df%>%
    slice(-1)%>%# drop first row
    slice(-(33:nrow(nfl_df)))%>% # drop summary data at end
    mutate(year=year)%>% # add year
    ## Convert to numeric
    mutate_at(.funs = as.numeric,.vars=names(nfl_df)[3:dim(nfl_df)[2]])
  
## Win/loss record

## Parse web page
nfl_afc<-url %>% read_html() %>%                   # parse html
      html_nodes('#AFC')%>%
      html_table()%>%
      data.frame()%>%
      slice(-c(seq(1,20,by=5)))
      

## Parse web page
nfl_nfc<-url %>% read_html() %>%                   # parse html
      html_nodes('#NFC')%>%
      html_table()%>%
      data.frame()%>%
      slice(-c(seq(1,20,by=5)))

nfl_record<-bind_rows(nfl_afc,nfl_nfc)

select_vars<-c("Tm","W","L","PF","PA")

nfl_record<-nfl_record%>%select_at(.vars=select_vars)

names(nfl_record)<-c(
                     "team",
                     "wins",
                     "losses",
                     "points_for",
                     "points_against")

nfl_record<-as_data_frame(nfl_record)%>%tbl_df()%>%
  mutate(team=str_remove_all(team,"[*+]"))%>%
  mutate_at(.funs=as.numeric,.vars=2:5)

nfl_df<-nfl_df%>%as_data_frame()%>%tbl_df()

nfl_df<-left_join(nfl_df,nfl_record,by="team")

    ## Combine
    nfl_df_all<-bind_rows(nfl_df_all,nfl_df)
    
    ## Wait between 3 and 4 seconds
    Sys.sleep(runif(1,3,4))
    
      print(paste("Finishing year:" ,year))
  } # End year loop
  
  
  save(nfl_df_all,file="nfl_df_all.Rdata")
} else{
    load("nfl_df_all.Rdata")
  } #end conditional
```

```{r}
gg<-ggplot(nfl_df_all,aes(x=rush_yards,
                          y=wins,
                          text=team,
                          color=as.factor(year)))
gg<-gg+geom_point()
gg<-gg+facet_wrap(~year)
gg<-gg+theme(legend.position="none")+xlab("Rush Yards")+ylab("Wins")
ggplotly(gg)

```


# Interacting via APIs

Many websites have created Application Programming Interfaces, which allow the user to directly communicate with the website's underlying database without dealing with the intermediary web content. These have been expanding rapdily and are one of the most exciting areas of development in data access for data science. 

Today, we'll be working with two APIs:  Zillow and the American Community Survey from the census. Please go to: `http://www.census.gov/developers/` and click on "Get a Key" to get your census key. Similarly, go to: `http://www.zillow.com/howto/api/APIOverview.htm` to get your key from Zillow. 

*YOU NEED TO PAY ATTENTION TO TERMS OF USE WHEN USING APIS. DO NOT VIOLATE THESE.*

With these keys in hand, we can interact with these various databases. Today, we're going to take a look at home prices as a function of income and education level for all of the zip codes in Davidson County TN. This section was inspired by this post: https://notesofdabbler.wordpress.com/2013/12/25/exploring-census-and-demographic-data-with-r/. 

## Zip Code Level Data from the American Community Survey

The first step is to create a list of all zip codes in Davidson County. We can do this by using another dataset that includes a comprehensive listing of zip codes by county and city. 

```{r}
## Look up fips code for county
lookup_code("TN","Davidson") 

state_fips<-"47"
county_stub<-"037"

county_fips<-paste0(state_fips,county_stub)

# Get dataset that matches all zip codes to cities, counties and states. 
county_to_zip<-read_csv("http://www2.census.gov/geo/docs/maps-data/data/rel/zcta_county_rel_10.txt")

#easier names to work with
names(county_to_zip)<-tolower(names(county_to_zip))

#Just zip codes in selected county
county_to_zip<-county_to_zip%>%
  filter(state%in%as.numeric(state_fips),county==county_stub)%>%
  select(zcta5,state,county)

#list of zip codes
ziplist<-county_to_zip$zcta5

#City names
data(zip_codes)

city_zip<-zip_codes%>%filter(zip%in%ziplist)%>%select(zip,city)

#Arrange in order
city_zip<-city_zip%>%arrange(as.numeric(zip))
```

Next, we'll turn to the American Community Survey. This includes a large number of tables (available here in excel file form:  https://www.census.gov/programs-surveys/acs/technical-documentation/summary-file-documentation.html) that cover many demographic and other characteristics of the population, down to the level of zip codes. We'll use the `acs` package to get two tables for the zip codes we're interested in: levels of education and income. We'll turn these tables into two variables: the proportion of the population with incomes above $75,000, and the proportion of the population with at least a bachelor's degree. 

The first step is to get the table from ACS. Below, I submit a request using my key to get table B15002, which contains information on education levels. 

## Using Tidycensus
```{r}
my_acs_key<-readLines("~/hod_datasci_keys/my_acs_key.txt",warn = FALSE)

acs_key<-my_acs_key

# Or just paste it here.


#acs_key<-"<your_acs_key_here>"

census_api_key(acs_key)

v19 <- load_variables(2019, "acs5", cache = TRUE)

View(v19)

var_list<-paste0("B15002_",c("001",
                            "015",
                            "016",
                            "017",
                            "018",
                            "032",
                            "033",
                            "034",
                            "035"))

educ<-get_acs(geography="county",
              state="CA",
                variables=var_list,
                output="wide",
                geometry=TRUE,
              keep_geo_vars = TRUE,
              year = 2016
                )

names(educ)<-tolower(names(educ))

educ<-educ%>%
  mutate(zip=str_sub(name,start=7,end=11))


county_educ<-educ%>%
  filter(zip%in%ziplist)

county_educ<-
  county_educ%>%
  group_by(zip)%>%
  mutate(college_educ=((b15002_015e+
                         b15002_016e+
                         b15002_017e+
                         b15002_018e+
                         b15002_032e+
                         b15002_033e+
                         b15002_034e+
                         b15002_035e)/b15002_001e)*100) %>%
    select(zip,college_educ)


```

```{r}
var_list<-paste0("B19001_",c("001",
                             "013",
                             "014",
                            "015",
                            "016",
                            "017"
                            ))

income<-get_acs("zcta",
                variables=var_list,
                output="wide",
                year=2016
                )

names(income)<-tolower(names(income))


income<-income%>%
  mutate(zip=str_sub(name,start=7,end=11))


county_income<-
  income%>%
  filter(zip%in%ziplist)

county_income %>%
group_by(zip) %>%
  mutate(
    income_75 = ((
      b19001_013e +
        b19001_014e +
        b19001_015e +
        b19001_016e +
        b19001_017e 
    ) / b19001_001e)*100
  )%>%
  select(zip,income_75)->
  county_income

```

```{r}
county_df<-geo_join(county_educ,county_income,by="zip")
county_df<-geo_join(county_df,city_zip,by='zip')
county_df%>%
  select(city,zip,college_educ,income_75)%>%
  arrange(zip)->county_df
```



_Quick Exercise_ Pull table B23001 "Sex by Age by Employment Status for the Population 16 and over" from ACS. 


## Mapping

The last set of code maps these zip codes. We will use two tools: `tigris` will help us to interact with the Census Bureau's `tiger` shapefile data, and `leaflet` will help us draw interactive maps.


## Tigris library and shapefiles

To map something, you need to get the shapefile that's associate with the geogrpahic units we're interested in. We've already got the shapefile associated with zip codes in TN. (Zip codes are tehcnically routes and not geographic areas, but shut up, geography nerds.) 

For the map we're going to create, I want text to pop up that will show the characteristics of each Zip code in Davidson county. 

#Mapping: simple(ish) leaflet map
```{r}
county_df<-county_df%>%
  st_collection_extract("LINESTRING")

pal <- colorQuantile(palette = "viridis", 
                     domain = county_df$college_educ, n = 10)

county_df%>%
    st_transform(crs = "+init=epsg:4326")%>%
    leaflet(width = "100%") %>%
    addProviderTiles(provider = "CartoDB.Positron") %>%
    addPolygons(popup = ~ zip, 
                stroke = FALSE,
                smoothFactor = 0,
                fillOpacity = 0.7,
                color = ~ pal(college_educ)) %>%
    addLegend("bottomright", 
              pal = pal, 
              values = ~ college_educ,
              title = "Percent with a bachelor's degree",
              opacity = 1)

```


```{r}
popup <- paste0("Zip: ", county_df$zip,
                "<br>",
                 county_df$city,
                "<br>",
                "Percent of Pop with A Bachelors: ",
                round((county_df$college_educ),1),
                "<br>",
                "Percent of Pop with income > 75: ",
                round(county_df$income_75,1)
          )
```

Next, we'll need a palette that will map to each of the scales. The `colorNumeric` palette function takes a "palette" from RColorBrewer and maps it to a domain defined by the varible. 

```{r}

#Palette for Education
educ_pal <- colorNumeric(
  palette = "YlGnBu",
  domain = county_df$college_educ,
    na.color="gray"
)

#Palette for Income
income_pal <- colorNumeric(
  palette = "YlGnBu",
  domain = county_df$income_75,
    na.color="gray"
)
```

## Mapping using leaflet

Leaflet is a powerful tool based on Javascript. It's implemented in R via the leaflet library. The way this works is in layers, much like ggplot. Each `addPolygons` command maps each zip code according to its own palette. The `addProviderTiles` adds in the underlying roadmap, while the `addLayerControl` provides access to the interface for showing different layers. 

## More complex leaflet map
```{r}
county_df%>%
  st_transform(crs = "+init=epsg:4326")->county_df


  leaflet() %>%
  # underlying map from Carto.DB
  addProviderTiles("CartoDB.Positron") %>%
  # Values for education  
    addPolygons(data = county_df, 
              fillColor = ~educ_pal(college_educ), 
              color = "#b2aeae", # you need to use hex colors
              fillOpacity = 0.7, 
              weight = 1, 
              smoothFactor = 0.2,
              popup = popup,
              group="Education"
              ) %>%
  # Values for income  
      addPolygons(data = county_df, 
              fillColor = ~income_pal(county_df$income_75), 
              color = "#b2aeae", # you need to use hex colors
              fillOpacity = 0.7, 
              weight = 1, 
              smoothFactor = 0.2,
              popup = popup,
              group="Income"
              ) %>%
  #Hide everying but Education to start
    hideGroup(c("Income"))%>%
  # Add layers control, so user can select different layers
    addLayersControl(
    overlayGroups = c("Education","Income"),
    options = layersControlOptions(collapsed = FALSE)
  )
```
## Nashville Neighborhoods
```{r}
map
```
*Note: Use the box in the upper right corner to select different attributes of neighborhoods*

Nashville is a city with very sharply demarcated neighborhoods. High income, high SES neighborhoods feature very high home prices and high prices per square foot, while areas with lower levels of education and income have much less robust housing markets. 


## OMDB Data
The code below first gets the titles of every Studio Ghibli movie, then searches the OMDB for these movies. It uses the "map" function to avoid loops

```{r}

#install.packages("devtools")

devtools::install_github("hrbrmstr/omdbapi")

library(omdbapi)
mykey<-read_lines("~/hod_datasci_keys/omdb_key.txt")

"bb6d7863"

omdb_api_key(mykey)

url<-"https://en.wikipedia.org/wiki/List_of_Studio_Ghibli_works"

# Get the page, pull the tables via html_table
titles<- read_html(url)%>%html_table(fill=TRUE)

# Titles is the second table
titles<-tbl_df(data.frame(titles[[2]]))%>%
  dplyr::select(Title)%>%as_vector()

search_by_title("Ponyo")

# Search IMDB for each title, add to data frame
mv<-map_df(titles,search_by_title)%>%
  filter(Title%in%titles)

# Keep just the first version of each movie (particular to this example) 
mv<-mv%>%group_by(Title)%>%
  filter(row_number()==1)

# Send request for these IDs to the database
mv_df <- map_df(mv$imdbID, find_by_id,include_tomatoes=TRUE)

# pull rating source
mv_df$rate_source<-mv_df$Ratings%>%map(1)%>%as_vector()

# pull rating
mv_df$rating<-mv_df$Ratings%>%map(2)%>%as_vector()

# 
mv_df<-mv_df%>%
  filter(rate_source=="Rotten Tomatoes")%>%
    mutate(rating=parse_number(rating))

```


```{r}
gg<-
mv_df%>%
ggplot(aes(x=Year,y=imdbRating,text=Title,color=Title))+
geom_point()+
  theme(legend.position = "none")

ggplotly(gg)
```


```{r}

url<-"https://en.wikipedia.org/wiki/Academy_Award_for_Best_Picture"

# Get the page, pull the tables via html_table
titles<- read_html(url)%>%
  html_nodes('table')

titles<-titles[[12]] %>%
  html_table(fill=TRUE)%>%
  mutate(Year=str_sub(Year,start=1,end=4))%>%
  filter(Year>2010)%>%
  select(Film)%>%as_vector()

# Search IMDB for each title, add to data frame
mv<-map_df(titles,search_by_title)%>%
  filter(Title%in%titles)%>%
  filter(Type=="movie")

# Keep just the first version of each movie (particular to this example) 
mv<-mv%>%group_by(Title)%>%
  filter(row_number()==1)

# Send request for these IDs to the omdb database
mv_df <- map_df(mv$imdbID, find_by_id,include_tomatoes=TRUE)

# pull rating source
mv_df$rate_source<-mv_df$Ratings%>%map(1)%>%as_vector()

# pull rating
mv_df$rating<-mv_df$Ratings%>%map(2)%>%as_vector()

# Get relevant ino
mv_df<-mv_df%>%
  filter(rate_source=="Rotten Tomatoes")%>%
  mutate(rating=parse_number(rating))%>%
  mutate(BoxOffice=parse_number(BoxOffice))%>%
  mutate(imdbRating=parse_number(imdbRating))
```



```{r}
gg<-ggplot(mv_df,aes(x=BoxOffice,y=rating,text=Title))
gg<-gg+geom_point(aes(color=Year))
gg<-gg+scale_x_continuous(trans="log")
ggplotly(gg)
```
